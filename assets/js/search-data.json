{
  
    
        "post0": {
            "title": "Data_science_snippets",
            "content": "Pandas cheat sheet for data science . Pandas cheat sheet for data science | Statistics Multi-variate analysis | Feature understanding | . | Preliminaries | Import | Input Output Input Reading files | . | Output | . | Exploration | Selecting Summary | Whole DataFrame | Columns | Rows | Cells | . | Data wrangling Merge Join | GroupBy | Dates | Missing data | Categorical Data | Manipulations Method chaining | Binning | . | fast append to dataframe | . | Performance Reshaping dataframe | Concat vs. append | Dataframe: iterate rows Useful links | . | Parallel data structures | . | Jupyter notebooks Notebooks in production workflow | Directory structure | Further link | jupyter notebook template header section | . | Orchestration | code snippets | . | Timing and Profiling Testing | Qgrid | Debugging conda | Running Jupyter | installing kernels | . | . | . generated with markdown-toc . Statistics . Multi-variate analysis . Understand the problem. . Normal distribution python #histogram sns.distplot(df_train[‘SalePrice’]); . | Skewness/Kurtosis? #skewness and kurtosis print(&quot;Skewness: %f&quot; % df_train[&#39;SalePrice&#39;].skew()) print(&quot;Kurtosis: %f&quot; % df_train[&#39;SalePrice&#39;].kurt()) . | Show peakedness | . Univariable study . Relationship with numerical variables . #scatter plot grlivarea/saleprice var = &#39;GrLivArea&#39; data = pd.concat([df_train[&#39;SalePrice&#39;], df_train[var]], axis=1) data.plot.scatter(x=var, y=&#39;SalePrice&#39;, ylim=(0,800000)); . Relationship with categorical features . #box plot overallqual/saleprice var = &#39;OverallQual&#39; data = pd.concat([df_train[&#39;SalePrice&#39;], df_train[var]], axis=1) f, ax = plt.subplots(figsize=(8, 6)) fig = sns.boxplot(x=var, y=&quot;SalePrice&quot;, data=data) fig.axis(ymin=0, ymax=800000); . Multivariate study. We’ll try to understand how the dependent variable and independent variables relate. | Correlation matrix (heatmap style) . #correlation matrix corrmat = df_train.corr() f, ax = plt.subplots(figsize=(12, 9)) sns.heatmap(corrmat, vmax=.8, square=True); # and zoomed corr matrix k = 10 #number of variables for heatmap cols = corrmat.nlargest(k, &#39;SalePrice&#39;)[&#39;SalePrice&#39;].index cm = np.corrcoef(df_train[cols].values.T) sns.set(font_scale=1.25) hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt=&#39;.2f&#39;, annot_kws={&#39;size&#39;: 10}, yticklabels=cols.values, xticklabels=cols.values) plt.show() . . Scatter plots between target and correlated variables . #scatterplot sns.set() cols = [&#39;SalePrice&#39;, &#39;OverallQual&#39;, &#39;GrLivArea&#39;, &#39;GarageCars&#39;, &#39;TotalBsmtSF&#39;, &#39;FullBath&#39;, &#39;YearBuilt&#39;] sns.pairplot(df_train[cols], size = 2.5) plt.show(); . Basic cleaning. We’ll clean the dataset and handle the missing data, outliers and categorical variables. . #missing data total = df_train.isnull().sum().sort_values(ascending=False) percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False) missing_data = pd.concat([total, percent], axis=1, keys=[&#39;Total&#39;, &#39;Percent&#39;]) missing_data.head(20) . Test assumptions. We’ll check if our data meets the assumptions required by most multivariate techniques. . Feature understanding . Are the features continuous, discrete or none of the above? . | What is the distribution of this feature? . | Does the distribution largely depend on what subset of examples is being considered? Time-based segmentation? | Type-based segmentation? | . | Does this feature contain holes (missing values)? Are those holes possible to be filled, or would they stay forever? | If it possible to eliminate them in the future data? | . | Are there duplicate and/or intersecting examples? Answering this question right is extremely important, since duplicate or connected data points might significantly affect the results of model validation if not properly excluded. | . | Where do the features come from? Should we come up with the new features that prove to be useful, how hard would it be to incorporate those features in the final design? | . | Is the data real-time? Are the requests real-time? | . | If yes, well-engineered simple features would likely rock. If no, we likely are in the business of advanced models and algorithms. | . | Are there features that can be used as the “truth”? Plots . Supervised vs. unsupervised learning | classification vs. regression | Prediction vs. Inference 9. Baseline Modeling 10. Secondary Modeling 11. Communicating Results 12. Conclusion 13. Resources | | . Preliminaries . # import libraries (standard) import numpy as np import matplotlib.pyplot as plt import pandas as pd from pandas import DataFrame, Series . Import . https://chrisyeh96.github.io/2017/08/08/definitive-guide-python-imports.html . Input Output . Input . Empty DataFrame (top) . # dataframe empty df = DataFrame() . CSV (top) . # pandas read csv df = pd.read_csv(&#39;file.csv&#39;) df = pd.read_csv(&#39;file.csv&#39;, header=0, index_col=0, quotechar=&#39;&quot;&#39;,sep=&#39;:&#39;, na_values = [&#39;na&#39;, &#39;-&#39;, &#39;.&#39;, &#39;&#39;]) # specifying &quot;.&quot; and &quot;NA&quot; as missing values in the Last Name column and &quot;.&quot; as missing values in Pre-Test Score column df = pd.read_csv(&#39;../data/example.csv&#39;, na_values={&#39;Last Name&#39;: [&#39;.&#39;, &#39;NA&#39;], &#39;Pre-Test Score&#39;: [&#39;.&#39;]}) # skipping the top 3 rows df = pd.read_csv(&#39;../data/example.csv&#39;, na_values=sentinels, skiprows=3) # interpreting &quot;,&quot; in strings around numbers as thousands separators df = pd.read_csv(&#39;../data/example.csv&#39;, thousands=&#39;,&#39;) # `encoding=&#39;latin1&#39;`, `encoding=&#39;iso-8859-1&#39;` or `encoding=&#39;cp1252&#39;` df = pd.read_csv(&#39;example.csv&#39;,encoding=&#39;latin1&#39;) . CSV (Inline) (top) . # pandas read string from io import StringIO data = &quot;&quot;&quot;, Animal, Cuteness, Desirable row-1, dog, 8.7, True row-2, cat, 9.5, True row-3, bat, 2.6, False&quot;&quot;&quot; df = pd.read_csv(StringIO(data), header=0, index_col=0, skipinitialspace=True) . JSON (top) . # pandas read json import json json_data = open(&#39;data-text.json&#39;).read() data = json.loads(json_data) for item in data: print item . XML (top) . # pandas read xml from xml.etree import ElementTree as ET tree = ET.parse(&#39;../../data/chp3/data-text.xml&#39;) root = tree.getroot() print root data = root.find(&#39;Data&#39;) all_data = [] for observation in data: record = {} for item in observation: lookup_key = item.attrib.keys()[0] if lookup_key == &#39;Numeric&#39;: rec_key = &#39;NUMERIC&#39; rec_value = item.attrib[&#39;Numeric&#39;] else: rec_key = item.attrib[lookup_key] rec_value = item.attrib[&#39;Code&#39;] record[rec_key] = rec_value all_data.append(record) print all_data . Excel (top) . # pandas read excel # Each Excel sheet in a Python dictionary workbook = pd.ExcelFile(&#39;file.xlsx&#39;) d = {} # start with an empty dictionary for sheet_name in workbook.sheet_names: df = workbook.parse(sheet_name) d[sheet_name] = df . MySQL (top) . # pandas read sql import pymysql from sqlalchemy import create_engine engine = create_engine(&#39;mysql+pymysql://&#39; +&#39;USER:PASSWORD@HOST/DATABASE&#39;) df = pd.read_sql_table(&#39;table&#39;, engine) . Combine DataFrame (top) . # pandas concat dataframes # Example 1 ... s1 = Series(range(6)) s2 = s1 * s1 s2.index = s2.index + 2# misalign indexes df = pd.concat([s1, s2], axis=1) # Example 2 ... s3 = Series({&#39;Tom&#39;:1, &#39;Dick&#39;:4, &#39;Har&#39;:9}) s4 = Series({&#39;Tom&#39;:3, &#39;Dick&#39;:2, &#39;Mar&#39;:5}) df = pd.concat({&#39;A&#39;:s3, &#39;B&#39;:s4 }, axis=1) . From Dictionary (top) default — assume data is in columns . # pandas read dictionary df = DataFrame({ &#39;col0&#39; : [1.0, 2.0, 3.0, 4.0], &#39;col1&#39; : [100, 200, 300, 400] }) . use helper method for data in rows . # pandas read dictionary df = DataFrame.from_dict({ # data by row # rows as python dictionaries &#39;row0&#39; : {&#39;col0&#39;:0, &#39;col1&#39;:&#39;A&#39;}, &#39;row1&#39; : {&#39;col0&#39;:1, &#39;col1&#39;:&#39;B&#39;} }, orient=&#39;index&#39;) df = DataFrame.from_dict({ # data by row # rows as python lists &#39;row0&#39; : [1, 1+1j, &#39;A&#39;], &#39;row1&#39; : [2, 2+2j, &#39;B&#39;] }, orient=&#39;index&#39;) . from iterations of lists . # pandas read lists aa = [&#39;aa1&#39;, &#39;aa2&#39;, &#39;aa3&#39;, &#39;aa4&#39;, &#39;aa5&#39;] bb = [&#39;bb1&#39;, &#39;bb2&#39;, &#39;bb3&#39;, &#39;bb4&#39;, &#39;bb5&#39;] cc = [&#39;cc1&#39;, &#39;cc2&#39;, &#39;cc3&#39;, &#39;cc4&#39;, &#39;cc5&#39;] lists = [aa, bb, cc] pd.DataFrame(list(itertools.product(*lists)), columns=[&#39;aa&#39;, &#39;bb&#39;, &#39;cc&#39;]) . source: https://stackoverflow.com/questions/45672342/create-a-dataframe-of-permutations-in-pandas-from-list . Examples (top) — simple - default integer indexes . # pandas read random df = DataFrame(np.random.rand(50,5)) . — with a time-stamp row index: . # pandas read random timestamp df = DataFrame(np.random.rand(500,5)) df.index = pd.date_range(&#39;1/1/2005&#39;, periods=len(df), freq=&#39;M&#39;) . — with alphabetic row and col indexes and a “groupable” variable . import string import random r = 52 # note: min r is 1; max r is 52 c = 5 df = DataFrame(np.random.randn(r, c), columns = [&#39;col&#39;+str(i) for i in range(c)], index = list((string. ascii_uppercase+ string.ascii_lowercase)[0:r])) df[&#39;group&#39;] = list(&#39;&#39;.join(random.choice(&#39;abcde&#39;) for _ in range(r)) ) . Generate dataframe with 1 variable column . # pandas dataframe create final_biom_df = final_biom_df.append([pd.DataFrame({&#39;trial&#39; : curr_trial, &#39;biomarker_name&#39; : curr_biomarker, &#39;observation_id&#39; : curr_observation, &#39;visit&#39; : curr_timepoint, &#39;value&#39; : np.random.randint(low=1, high=100, size=30), &#39;unit&#39; : curr_unit, &#39;base&#39; : is_base, })]) . Reading files . # pandas read multiple files files = glob.glob(&#39;weather/*.csv&#39;) weather_dfs = [pd.read_csv(fp, names=columns) for fp in files] weather = pd.concat(weather_dfs) . Output . CSV (top) . df.to_csv(&#39;name.csv&#39;, encoding=&#39;utf-8&#39;) df.to_csv(&#39;filename.csv&#39;, header=False) . Excel . from pandas import ExcelWriter writer = ExcelWriter(&#39;filename.xlsx&#39;) df1.to_excel(writer,&#39;Sheet1&#39;) df2.to_excel(writer,&#39;Sheet2&#39;) writer.save() . MySQL (top) . import pymysql from sqlalchemy import create_engine e = create_engine(&#39;mysql+pymysql://&#39; + &#39;USER:PASSWORD@HOST/DATABASE&#39;) df.to_sql(&#39;TABLE&#39;,e, if_exists=&#39;replace&#39;) . Python object (top) . d = df.to_dict() # to dictionary str = df.to_string() # to string m = df.as_matrix() # to numpy matrix . JSON . ### orient=’records’ df.to_json(r&#39;Path to store the JSON file File Name.json&#39;,orient=&#39;records&#39;) [{&quot;Product&quot;:&quot;Desktop Computer&quot;,&quot;Price&quot;:700},{&quot;Product&quot;:&quot;Tablet&quot;,&quot;Price&quot;:250},{&quot;Produc . source: https://datatofish.com/export-pandas-dataframe-json/ . Exploration . pandas profiling . conda install -c anaconda pandas-profiling import pandas as pd import pandas_profiling # Depreciated: pre 2.0.0 version df = pd.read_csv(&#39;titanic/train.csv&#39;) #Pandas-Profiling 2.0.0 df.profile_report() # save as html profile = df.profile_report(title=&#39;Pandas Profiling Report&#39;) profile.to_file(output_file=&quot;output.html&quot;) . example report: link . source: link . overview missing data: . # dataframe missing data ted.isna().sum() . Selecting . Summary . Select columns . # dataframe select columns s = df[&#39;col_label&#39;] # returns Series df = df[[&#39;col_label&#39;]] # return DataFrame df = df[[&#39;L1&#39;, &#39;L2&#39;]] # select with list df = df[index] # select with index df = df[s] #select with Series . Select rows . # dataframe select rows df = df[&#39;from&#39;:&#39;inc_to&#39;]# label slice df = df[3:7] # integer slice df = df[df[&#39;col&#39;] &gt; 0.5]# Boolean Series df = df.loc[&#39;label&#39;] # single label df = df.loc[container] # lab list/Series df = df.loc[&#39;from&#39;:&#39;to&#39;]# inclusive slice df = df.loc[bs] # Boolean Series df = df.iloc[0] # single integer df = df.iloc[container] # int list/Series df = df.iloc[0:5] # exclusive slice df = df.ix[x] # loc then iloc . Select a cross-section (top) . # dataframe select slices # r and c can be scalar, list, slice df.loc[r, c] # label accessor (row, col) df.iloc[r, c]# integer accessor df.ix[r, c] # label access int fallback df[c].iloc[r]# chained – also for .loc . Select a cell (top) . # dataframe select cell # r and c must be label or integer df.at[r, c] # fast scalar label accessor df.iat[r, c] # fast scalar int accessor df[c].iat[r] # chained – also for .at . DataFrame indexing methods (top) . v = df.get_value(r, c) # get by row, col df = df.set_value(r,c,v)# set by row, col df = df.xs(key, axis) # get cross-section df = df.filter(items, like, regex, axis) df = df.select(crit, axis) . Some index attributes and methods (top) . # dataframe index atrributes # some Index attributes b = idx.is_monotonic_decreasing b = idx.is_monotonic_increasing b = idx.has_duplicates i = idx.nlevels # num of index levels # some Index methods idx = idx.astype(dtype)# change data type b = idx.equals(o) # check for equality idx = idx.union(o) # union of two indexes i = idx.nunique() # number unique labels label = idx.min() # minimum label label = idx.max() # maximum label . Whole DataFrame . Content/Structure . # dataframe get info df.info() # index &amp; data types dfh = df.head(i) # get first i rows dft = df.tail(i) # get last i rows dfs = df.describe() # summary stats cols top_left_corner_df = df.iloc[:4, :4] . Non-indexing attributes (top) . # dataframe non-indexing methods dfT = df.T # transpose rows and cols l = df.axes # list row and col indexes (r, c) = df.axes # from above s = df.dtypes # Series column data types b = df.empty # True for empty DataFrame i = df.ndim # number of axes (it is 2) t = df.shape # (row-count, column-count) i = df.size # row-count * column-count a = df.values # get a numpy array for df . Utilities - DataFrame utility methods (top) . # dataframe sort df = df.copy() # dataframe copy df = df.rank() # rank each col (default) df = df.sort([&#39;sales&#39;], ascending=[False]) df = df.sort_values(by=col) df = df.sort_values(by=[col1, col2]) df = df.sort_index() df = df.astype(dtype) # type conversion . Iterations (top) . # dataframe iterate for df.iteritems()# (col-index, Series) pairs df.iterrows() # (row-index, Series) pairs # example ... iterating over columns for (name, series) in df.iteritems(): print(&#39;Col name: &#39; + str(name)) print(&#39;First value: &#39; + str(series.iat[0]) + &#39; n&#39;) . Maths (top) . # dataframe math df = df.abs() # absolute values df = df.add(o) # add df, Series or value s = df.count() # non NA/null values df = df.cummax() # (cols default axis) df = df.cummin() # (cols default axis) df = df.cumsum() # (cols default axis) df = df.diff() # 1st diff (col def axis) df = df.div(o) # div by df, Series, value df = df.dot(o) # matrix dot product s = df.max() # max of axis (col def) s = df.mean() # mean (col default axis) s = df.median()# median (col default) s = df.min() # min of axis (col def) df = df.mul(o) # mul by df Series val s = df.sum() # sum axis (cols default) df = df.where(df &gt; 0.5, other=np.nan) . Select/filter (top) . # dataframe select filter df = df.filter(items=[&#39;a&#39;, &#39;b&#39;]) # by col df = df.filter(items=[5], axis=0) #by row df = df.filter(like=&#39;x&#39;) # keep x in col df = df.filter(regex=&#39;x&#39;) # regex in col df = df.select(lambda x: not x%5)#5th rows . Columns . Index and labels (top) . # dataframe get index idx = df.columns # get col index label = df.columns[0] # first col label l = df.columns.tolist() # list col labels . Data type conversions (top) . # dataframe convert column st = df[&#39;col&#39;].astype(str)# Series dtype a = df[&#39;col&#39;].values # numpy array pl = df[&#39;col&#39;].tolist() # python list . Note: useful dtypes for Series conversion: int, float, str . Common column-wide methods/attributes (top) . value = df[&#39;col&#39;].dtype # type of column value = df[&#39;col&#39;].size # col dimensions value = df[&#39;col&#39;].count()# non-NA count value = df[&#39;col&#39;].sum() value = df[&#39;col&#39;].prod() value = df[&#39;col&#39;].min() # column min value = df[&#39;col&#39;].max() # column max value = df[&#39;col&#39;].mean() # also median() value = df[&#39;col&#39;].cov(df[&#39;col2&#39;]) s = df[&#39;col&#39;].describe() s = df[&#39;col&#39;].value_counts() . Find index label for min/max values in column (top) . label = df[&#39;col1&#39;].idxmin() label = df[&#39;col1&#39;].idxmax() . Common column element-wise methods (top) . s = df[&#39;col&#39;].isnull() s = df[&#39;col&#39;].notnull() # not isnull() s = df[&#39;col&#39;].astype(float) s = df[&#39;col&#39;].abs() s = df[&#39;col&#39;].round(decimals=0) s = df[&#39;col&#39;].diff(periods=1) s = df[&#39;col&#39;].shift(periods=1) s = df[&#39;col&#39;].to_datetime() s = df[&#39;col&#39;].fillna(0) # replace NaN w 0 s = df[&#39;col&#39;].cumsum() s = df[&#39;col&#39;].cumprod() s = df[&#39;col&#39;].pct_change(periods=4) s = df[&#39;col&#39;].rolling_sum(periods=4, window=4) . Note: also rolling_min(), rolling_max(), and many more. . Position of a column index label (top) . j = df.columns.get_loc(&#39;col_name&#39;) . Column index values unique/monotonic (top) . if df.columns.is_unique: pass # ... b = df.columns.is_monotonic_increasing b = df.columns.is_monotonic_decreasing . Selecting . Columns (top) . s = df[&#39;colName&#39;] # select col to Series df = df[[&#39;colName&#39;]] # select col to df df = df[[&#39;a&#39;,&#39;b&#39;]] # select 2 or more df = df[[&#39;c&#39;,&#39;a&#39;,&#39;b&#39;]]# change col order s = df[df.columns[0]] # select by number df = df[df.columns[[0, 3, 4]] # by number s = df.pop(&#39;c&#39;) # get col &amp; drop from df . Columns with Python attributes (top) . s = df.a # same as s = df[&#39;a&#39;] # cannot create new columns by attribute df.existing_column = df.a / df.b df[&#39;new_column&#39;] = df.a / df.b . Selecting columns with .loc, .iloc and .ix (top) . df = df.loc[:, &#39;col1&#39;:&#39;col2&#39;] # inclusive df = df.iloc[:, 0:2] # exclusive . Conditional selection (top) . df.query(&#39;A &gt; C&#39;) df.query(&#39;A &gt; 0&#39;) df.query(&#39;A &gt; 0 &amp; A &lt; 1&#39;) df.query(&#39;A &gt; B | A &gt; C&#39;) df[df[&#39;coverage&#39;] &gt; 50] # all rows where coverage is more than 50 df[(df[&#39;deaths&#39;] &gt; 500) | (df[&#39;deaths&#39;] &lt; 50)] df[(df[&#39;score&#39;] &gt; 1) &amp; (df[&#39;score&#39;] &lt; 5)] df[~(df[&#39;regiment&#39;] == &#39;Dragoons&#39;)] # Select all the regiments not named &quot;Dragoons&quot; df[df[&#39;age&#39;].notnull() &amp; df[&#39;sex&#39;].notnull()] # ignore the missing data points . (top) . # is in df[df.name.isin(value_list)] # value_list = [&#39;Tina&#39;, &#39;Molly&#39;, &#39;Jason&#39;] df[~df.name.isin(value_list)] . Partial matching (top) . # column contains df2[df2.E.str.contains(&quot;tw|ou&quot;)] # column contains regex df[&#39;raw&#39;].str.contains(&#39;....-..-..&#39;, regex=True) # regex # dataframe column list contains selection = [&#39;cat&#39;, &#39;dog&#39;] df[pd.DataFrame(df.species.tolist()).isin(selection).any(1)] Out[64]: molecule species 0 a [dog] 2 c [cat, dog] 3 d [cat, horse, pig] . # dataframe column rename df.rename(columns={&#39;old1&#39;:&#39;new1&#39;,&#39;old2&#39;:&#39;new2&#39;}, inplace=True) df.columns = [&#39;a&#39;, &#39;b&#39;] . Manipulating . Adding (top) . df[&#39;new_col&#39;] = range(len(df)) df[&#39;new_col&#39;] = np.repeat(np.nan,len(df)) df[&#39;random&#39;] = np.random.rand(len(df)) df[&#39;index_as_col&#39;] = df.index df1[[&#39;b&#39;,&#39;c&#39;]] = df2[[&#39;e&#39;,&#39;f&#39;]] df3 = df1.append(other=df2) . Vectorised arithmetic on columns (top) . df[&#39;proportion&#39;]=df[&#39;count&#39;]/df[&#39;total&#39;] df[&#39;percent&#39;] = df[&#39;proportion&#39;] * 100.0 . Append a column of row sums to a DataFrame (top) . df[&#39;Total&#39;] = df.sum(axis=1) . Apply numpy mathematical functions to columns (top) . df[&#39;log_data&#39;] = np.log(df[&#39;col1&#39;]) . Set column values set based on criteria (top) . df[&#39;b&#39;]=df[&#39;a&#39;].where(df[&#39;a&#39;]&gt;0,other=0) df[&#39;d&#39;]=df[&#39;a&#39;].where(df.b!=0,other=df.c) . Swapping (top) . df[[&#39;B&#39;, &#39;A&#39;]] = df[[&#39;A&#39;, &#39;B&#39;]] . Dropping (top) . df = df.drop(&#39;col1&#39;, axis=1) df.drop(&#39;col1&#39;, axis=1, inplace=True) df = df.drop([&#39;col1&#39;,&#39;col2&#39;], axis=1) s = df.pop(&#39;col&#39;) # drops from frame del df[&#39;col&#39;] # even classic python works df.drop(df.columns[0], inplace=True) # drop columns with column names where the first three letters of the column names was &#39;pre&#39; cols = [c for c in df.columns if c.lower()[:3] != &#39;pre&#39;] df=df[cols] . Multiply every column in DataFrame by Series (top) . df = df.mul(s, axis=0) # on matched rows . Rows . Get Position (top) . a = np.where(df[&#39;col&#39;] &gt;= 2) #numpy array . DataFrames have same row index (top) . len(a)==len(b) and all(a.index==b.index) # Get the integer position of a row or col index label i = df.index.get_loc(&#39;row_label&#39;) . Row index values are unique/monotonic (top) . if df.index.is_unique: pass # ... b = df.index.is_monotonic_increasing b = df.index.is_monotonic_decreasing . Get the row index and labels (top) . idx = df.index # get row index label = df.index[0] # 1st row label lst = df.index.tolist() # get as a list . Change the (row) index (top) . df.index = idx # new ad hoc index df = df.set_index(&#39;A&#39;) # col A new index df = df.set_index([&#39;A&#39;, &#39;B&#39;]) # MultiIndex df = df.reset_index() # replace old w new . df.index = range(len(df)) # set with list df = df.reindex(index=range(len(df))) df = df.set_index(keys=[&#39;r1&#39;,&#39;r2&#39;,&#39;etc&#39;]) df.rename(index={&#39;old&#39;:&#39;new&#39;},inplace=True) . Selecting . By column values (top) . df = df[df[&#39;col2&#39;] &gt;= 0.0] df = df[(df[&#39;col3&#39;]&gt;=1.0) | (df[&#39;col1&#39;]&lt;0.0)] df = df[df[&#39;col&#39;].isin([1,2,5,7,11])] df = df[~df[&#39;col&#39;].isin([1,2,5,7,11])] df = df[df[&#39;col&#39;].str.contains(&#39;hello&#39;)] . Using isin over multiple columns (top) . ## fake up some data data = {1:[1,2,3], 2:[1,4,9], 3:[1,8,27]} df = DataFrame(data) # multi-column isin lf = {1:[1, 3], 3:[8, 27]} # look for f = df[df[list(lf)].isin(lf).all(axis=1)] Selecting rows using an index idx = df[df[&#39;col&#39;] &gt;= 2].index print(df.ix[idx]) . Slice of rows by integer position (top) . [inclusive-from : exclusive-to [: step]] default start is 0; default end is len(df) df = df[:] # copy DataFrame df = df[0:2] # rows 0 and 1 df = df[-1:] # the last row df = df[2:3] # row 2 (the third row) df = df[:-1] # all but the last row df = df[::2] # every 2nd row (0 2 ..) . Slice of rows by label/index (top) . [inclusive-from : inclusive–to [ : step]] df = df[&#39;a&#39;:&#39;c&#39;] # rows &#39;a&#39; through &#39;c&#39; . Manipulating . Adding rows . df = original_df.append(more_rows_in_df) . Append a row of column totals to a DataFrame (top) . # Option 1: use dictionary comprehension sums = {col: df[col].sum() for col in df} sums_df = DataFrame(sums,index=[&#39;Total&#39;]) df = df.append(sums_df) # Option 2: All done with pandas df = df.append(DataFrame(df.sum(), columns=[&#39;Total&#39;]).T) . Dropping rows (by name) (top) . df = df.drop(&#39;row_label&#39;) df = df.drop([&#39;row1&#39;,&#39;row2&#39;]) # multi-row . Drop duplicates in the row index (top) . df[&#39;index&#39;] = df.index # 1 create new col df = df.drop_duplicates(cols=&#39;index&#39;,take_last=True)# 2 use new col del df[&#39;index&#39;] # 3 del the col df.sort_index(inplace=True)# 4 tidy up . Iterating over DataFrame rows (top) . for (index, row) in df.iterrows(): # pass . Sorting . Rows values (top) . df = df.sort(df.columns[0], ascending=False) df.sort([&#39;col1&#39;, &#39;col2&#39;], inplace=True) . By row index (top) . df.sort_index(inplace=True) # sort by row df = df.sort_index(ascending=False) . Random (top) . import random as r k = 20 # pick a number selection = r.sample(range(len(df)), k) df_sample = df.iloc[selection, :] . df.take(np.random.permutation(len(df))[:3]) . Cells . Selecting . By row and column (top) . value = df.at[&#39;row&#39;, &#39;col&#39;] value = df.loc[&#39;row&#39;, &#39;col&#39;] value = df[&#39;col&#39;].at[&#39;row&#39;] # tricky . Note: .at[] fastest label based scalar lookup . By integer position (top) . value = df.iat[9, 3] # [row, col] value = df.iloc[0, 0] # [row, col] value = df.iloc[len(df)-1, len(df.columns)-1] . Slice by labels (top) . df = df.loc[&#39;row1&#39;:&#39;row3&#39;, &#39;col1&#39;:&#39;col3&#39;] . Slice by Integer Position (top) . df = df.iloc[2:4, 2:4] # subset of the df df = df.iloc[:5, :5] # top left corner s = df.iloc[5, :] # returns row as Series df = df.iloc[5:6, :] # returns row as row . By label and/or Index (top) . value = df.ix[5, &#39;col1&#39;] df = df.ix[1:5, &#39;col1&#39;:&#39;col3&#39;] . Manipulating . Setting a cell by row and column labels (top) . # pandas update df.at[&#39;row&#39;, &#39;col&#39;] = value df.loc[&#39;row&#39;, &#39;col&#39;] = value df[&#39;col&#39;].at[&#39;row&#39;] = value # tricky . Setting a cross-section by labels . df.loc[&#39;A&#39;:&#39;C&#39;, &#39;col1&#39;:&#39;col3&#39;] = np.nan df.loc[1:2,&#39;col1&#39;:&#39;col2&#39;]=np.zeros((2,2)) df.loc[1:2,&#39;A&#39;:&#39;C&#39;]=othr.loc[1:2,&#39;A&#39;:&#39;C&#39;] . Setting cell by integer position . df.iloc[0, 0] = value # [row, col] df.iat[7, 8] = value . Setting cell range by integer position . df.iloc[0:3, 0:5] = value df.iloc[1:3, 1:4] = np.ones((2, 3)) df.iloc[1:3, 1:4] = np.zeros((2, 3)) df.iloc[1:3, 1:4] = np.array([[1, 1, 1],[2, 2, 2]]) . Data wrangling . Merge Join . More examples: https://www.geeksforgeeks.org/python-pandas-merging-joining-and-concatenating/ . . (top) . Three ways to join two DataFrames: . merge (a database/SQL-like join operation) | concat (stack side by side or one on top of the other) | combine_first (splice the two together, choosing values from one over the other) | . # pandas merge # Merge on indexes df_new = pd.merge(left=df1, right=df2, how=&#39;outer&#39;, left_index=True, right_index=True) # Merge on columns df_new = pd.merge(left=df1, right=df2, how=&#39;left&#39;, left_on=&#39;col1&#39;, right_on=&#39;col2&#39;) # Join on indexes (another way of merging) df_new = df1.join(other=df2, on=&#39;col1&#39;,how=&#39;outer&#39;) df_new = df1.join(other=df2,on=[&#39;a&#39;,&#39;b&#39;],how=&#39;outer&#39;) # Simple concatenation is often the best # pandas concat df=pd.concat([df1,df2],axis=0)#top/bottom df = df1.append([df2, df3]) #top/bottom df=pd.concat([df1,df2],axis=1)#left/right # Combine_first (&lt;a href=&quot;#top&quot;&gt;top&lt;/a&gt;) df = df1.combine_first(other=df2) # multi-combine with python reduce() df = reduce(lambda x, y: x.combine_first(y), [df1, df2, df3, df4, df5]) . (top) . GroupBy . # pandas groupby # Grouping gb = df.groupby(&#39;cat&#39;) # by one columns gb = df.groupby([&#39;c1&#39;,&#39;c2&#39;]) # by 2 cols gb = df.groupby(level=0) # multi-index gb gb = df.groupby(level=[&#39;a&#39;,&#39;b&#39;]) # mi gb print(gb.groups) # Iterating groups – usually not needed # pandas groupby iterate for name, group in gb: print (name) print (group) # Selecting a group (&lt;a href=&quot;#top&quot;&gt;top&lt;/a&gt;) dfa = df.groupby(&#39;cat&#39;).get_group(&#39;a&#39;) dfb = df.groupby(&#39;cat&#39;).get_group(&#39;b&#39;) # pandas groupby aggregate # Applying an aggregating function # apply to a column ... s = df.groupby(&#39;cat&#39;)[&#39;col1&#39;].sum() s = df.groupby(&#39;cat&#39;)[&#39;col1&#39;].agg(np.sum) # apply to the every column in DataFrame s = df.groupby(&#39;cat&#39;).agg(np.sum) df_summary = df.groupby(&#39;cat&#39;).describe() df_row_1s = df.groupby(&#39;cat&#39;).head(1) # Applying multiple aggregating functions gb = df.groupby(&#39;cat&#39;) # apply multiple functions to one column dfx = gb[&#39;col2&#39;].agg([np.sum, np.mean]) # apply to multiple fns to multiple cols dfy = gb.agg({ &#39;cat&#39;: np.count_nonzero, &#39;col1&#39;: [np.sum, np.mean, np.std], &#39;col2&#39;: [np.min, np.max] }) Note: gb[&#39;col2&#39;] above is shorthand for df.groupby(&#39;cat&#39;)[&#39;col2&#39;], without the need for regrouping. # Transforming functions (&lt;a href=&quot;#top&quot;&gt;top&lt;/a&gt;) # pandas groupby function # transform to group z-scores, which have # a group mean of 0, and a std dev of 1. zscore = lambda x: (x-x.mean())/x.std() dfz = df.groupby(&#39;cat&#39;).transform(zscore) # pandas groupby fillna # replace missing data with group mean mean_r = lambda x: x.fillna(x.mean()) dfm = df.groupby(&#39;cat&#39;).transform(mean_r) # Applying filtering functions (&lt;a href=&quot;#top&quot;&gt;top&lt;/a&gt;) # select groups with more than 10 members eleven = lambda x: (len(x[&#39;col1&#39;]) &gt;= 11) df11 = df.groupby(&#39;cat&#39;).filter(eleven) # Group by a row index (non-hierarchical index) df = df.set_index(keys=&#39;cat&#39;) s = df.groupby(level=0)[&#39;col1&#39;].sum() dfg = df.groupby(level=0).sum() . (top) . Dates . # pandas timestamp # Dates and time – points and spans t = pd.Timestamp(&#39;2013-01-01&#39;) t = pd.Timestamp(&#39;2013-01-01 21:15:06&#39;) t = pd.Timestamp(&#39;2013-01-01 21:15:06.7&#39;) p = pd.Period(&#39;2013-01-01&#39;, freq=&#39;M&#39;) # pandas time series # A Series of Timestamps or Periods ts = [&#39;2015-04-01 13:17:27&#39;, &#39;2014-04-02 13:17:29&#39;] # Series of Timestamps (good) s = pd.to_datetime(pd.Series(ts)) # Series of Periods (often not so good) s = pd.Series( [pd.Period(x, freq=&#39;M&#39;) for x in ts] ) s = pd.Series(pd.PeriodIndex(ts,freq=&#39;S&#39;)) # From non-standard strings to Timestamps t = [&#39;09:08:55.7654-JAN092002&#39;, &#39;15:42:02.6589-FEB082016&#39;] s = pd.Series(pd.to_datetime(t, format=&quot;%H:%M:%S.%f-%b%d%Y&quot;)) # Dates and time – stamps and spans as indexes # pandas time periods date_strs = [&#39;2014-01-01&#39;, &#39;2014-04-01&#39;,&#39;2014-07-01&#39;, &#39;2014-10-01&#39;] dti = pd.DatetimeIndex(date_strs) pid = pd.PeriodIndex(date_strs, freq=&#39;D&#39;) pim = pd.PeriodIndex(date_strs, freq=&#39;M&#39;) piq = pd.PeriodIndex(date_strs, freq=&#39;Q&#39;) print (pid[1] - pid[0]) # 90 days print (pim[1] - pim[0]) # 3 months print (piq[1] - piq[0]) # 1 quarter time_strs = [&#39;2015-01-01 02:10:40.12345&#39;, &#39;2015-01-01 02:10:50.67890&#39;] pis = pd.PeriodIndex(time_strs, freq=&#39;U&#39;) df.index = pd.period_range(&#39;2015-01&#39;, periods=len(df), freq=&#39;M&#39;) dti = pd.to_datetime([&#39;04-01-2012&#39;], dayfirst=True) # Australian date format pi = pd.period_range(&#39;1960-01-01&#39;,&#39;2015-12-31&#39;, freq=&#39;M&#39;) # Hint: unless you are working in less than seconds, prefer PeriodIndex over DateTimeImdex. . # pandas converting times From DatetimeIndex to Python datetime objects (&lt;a href=&quot;#top&quot;&gt;top&lt;/a&gt;) dti = pd.DatetimeIndex(pd.date_range( start=&#39;1/1/2011&#39;, periods=4, freq=&#39;M&#39;)) s = Series([1,2,3,4], index=dti) na = dti.to_pydatetime() #numpy array na = s.index.to_pydatetime() #numpy array # From Timestamps to Python dates or times df[&#39;date&#39;] = [x.date() for x in df[&#39;TS&#39;]] df[&#39;time&#39;] = [x.time() for x in df[&#39;TS&#39;]] # From DatetimeIndex to PeriodIndex and back df = DataFrame(np.random.randn(20,3)) df.index = pd.date_range(&#39;2015-01-01&#39;, periods=len(df), freq=&#39;M&#39;) dfp = df.to_period(freq=&#39;M&#39;) dft = dfp.to_timestamp() # Working with a PeriodIndex (&lt;a href=&quot;#top&quot;&gt;top&lt;/a&gt;) pi = pd.period_range(&#39;1960-01&#39;,&#39;2015-12&#39;,freq=&#39;M&#39;) na = pi.values # numpy array of integers lp = pi.tolist() # python list of Periods sp = Series(pi)# pandas Series of Periods ss = Series(pi).astype(str) # S of strs ls = Series(pi).astype(str).tolist() # Get a range of Timestamps dr = pd.date_range(&#39;2013-01-01&#39;, &#39;2013-12-31&#39;, freq=&#39;D&#39;) # Error handling with dates # 1st example returns string not Timestamp t = pd.to_datetime(&#39;2014-02-30&#39;) # 2nd example returns NaT (not a time) t = pd.to_datetime(&#39;2014-02-30&#39;,coerce=True) # NaT like NaN tests True for isnull() b = pd.isnull(t) # --&gt; True # The tail of a time-series DataFrame (&lt;a href=&quot;#top&quot;&gt;top&lt;/a&gt;) df = df.last(&quot;5M&quot;) # the last five months . Upsampling and downsampling . # pandas upsample pandas downsample # upsample from quarterly to monthly pi = pd.period_range(&#39;1960Q1&#39;, periods=220, freq=&#39;Q&#39;) df = DataFrame(np.random.rand(len(pi),5), index=pi) dfm = df.resample(&#39;M&#39;, convention=&#39;end&#39;) # use ffill or bfill to fill with values # downsample from monthly to quarterly dfq = dfm.resample(&#39;Q&#39;, how=&#39;sum&#39;) . Time zones . # pandas time zones t = [&#39;2015-06-30 00:00:00&#39;,&#39;2015-12-31 00:00:00&#39;] dti = pd.to_datetime(t).tz_localize(&#39;Australia/Canberra&#39;) dti = dti.tz_convert(&#39;UTC&#39;) ts = pd.Timestamp(&#39;now&#39;, tz=&#39;Europe/London&#39;) # get a list of all time zones import pyzt for tz in pytz.all_timezones: print tz # Note: by default, Timestamps are created without timezone information. # Row selection with a time-series index # start with the play data above idx = pd.period_range(&#39;2015-01&#39;, periods=len(df), freq=&#39;M&#39;) df.index = idx february_selector = (df.index.month == 2) february_data = df[february_selector] q1_data = df[(df.index.month &gt;= 1) &amp; (df.index.month &lt;= 3)] mayornov_data = df[(df.index.month == 5) | (df.index.month == 11)] totals = df.groupby(df.index.year).sum() # The Series.dt accessor attribute t = [&#39;2012-04-14 04:06:56.307000&#39;, &#39;2011-05-14 06:14:24.457000&#39;, &#39;2010-06-14 08:23:07.520000&#39;] # a Series of time stamps s = pd.Series(pd.to_datetime(t)) print(s.dtype) # datetime64[ns] print(s.dt.second) # 56, 24, 7 print(s.dt.month) # 4, 5, 6 # a Series of time periods s = pd.Series(pd.PeriodIndex(t,freq=&#39;Q&#39;)) print(s.dtype) # datetime64[ns] print(s.dt.quarter) # 2, 2, 2 print(s.dt.year) # 2012, 2011, 2010 . Missing data . Missing data in a Series (top) . # pandas missing data series s = Series( [8,None,float(&#39;nan&#39;),np.nan]) #[8, NaN, NaN, NaN] s.isnull() #[False, True, True, True] s.notnull()#[True, False, False, False] s.fillna(0)#[8, 0, 0, 0] . # pandas missing data dataframe df = df.dropna() # drop all rows with NaN df = df.dropna(axis=1) # same for cols df=df.dropna(how=&#39;all&#39;) #drop all NaN row df=df.dropna(thresh=2) # drop 2+ NaN in r # only drop row if NaN in a specified col df = df.dropna(df[&#39;col&#39;].notnull()) . Recoding/Replacing missing data . # pandas fillna recoding replacing df.fillna(0, inplace=True) # np.nan -&gt; 0 s = df[&#39;col&#39;].fillna(0) # np.nan -&gt; 0 df = df.replace(r&#39; s+&#39;, np.nan,regex=True) # white space -&gt; np.nan # Non-finite numbers s = Series([float(&#39;inf&#39;), float(&#39;-inf&#39;),np.inf, -np.inf]) # Testing for finite numbers (&lt;a href=&quot;#top&quot;&gt;top&lt;/a&gt;) b = np.isfinite(s) . (top) . Categorical Data . # pandas categorical data s = Series([&#39;a&#39;,&#39;b&#39;,&#39;a&#39;,&#39;c&#39;,&#39;b&#39;,&#39;d&#39;,&#39;a&#39;], dtype=&#39;category&#39;) df[&#39;B&#39;] = df[&#39;A&#39;].astype(&#39;category&#39;) # Convert back to the original data type s = Series([&#39;a&#39;,&#39;b&#39;,&#39;a&#39;,&#39;c&#39;,&#39;b&#39;,&#39;d&#39;,&#39;a&#39;], dtype=&#39;category&#39;) s = s.astype(&#39;string&#39;) # Ordering, reordering and sorting s = Series(list(&#39;abc&#39;), dtype=&#39;category&#39;) print (s.cat.ordered) s=s.cat.reorder_categories([&#39;b&#39;,&#39;c&#39;,&#39;a&#39;]) s = s.sort() s.cat.ordered = False # Renaming categories (&lt;a href=&quot;#top&quot;&gt;top&lt;/a&gt;) s = Series(list(&#39;abc&#39;), dtype=&#39;category&#39;) s.cat.categories = [1, 2, 3] # in place s = s.cat.rename_categories([4,5,6]) # using a comprehension ... s.cat.categories = [&#39;Group &#39; + str(i) for i in s.cat.categories] # Adding new categories (&lt;a href=&quot;#top&quot;&gt;top&lt;/a&gt;) s = s.cat.add_categories([4]) # Removing categories (&lt;a href=&quot;#top&quot;&gt;top&lt;/a&gt;) s = s.cat.remove_categories([4]) s.cat.remove_unused_categories() #inplace . (top) . Manipulations and Cleaning . Conversions . # pandas convert to numeric ## errors=&#39;ignore&#39;` ## `errors=&#39;coerce` convert to `np.nan` ## mess up data invoices.loc[45612,&#39;Meal Price&#39;] = &#39;I am causing trouble&#39; invoices.loc[35612,&#39;Meal Price&#39;] = &#39;Me too&#39; # check if conversion worked invoices[&#39;Meal Price&#39;].apply(lambda x:type(x)).value_counts() **OUT: &lt;class &#39;int&#39;&gt; 49972 &lt;class &#39;str&#39;&gt; 2 # identify validating lines invoices[&#39;Meal Price&#39;][invoices[&#39;Meal Price&#39;].apply( lambda x: isinstance(x,str) )] . # convert messy numerical data ## convert the offending values into np.nan** invoices[&#39;Meal Price&#39;] = pd.to_numeric(invoices[&#39;Meal Price&#39;],errors=&#39;coerce&#39;) ## fill np.nan with the median of the data** invoices[&#39;Meal Price&#39;] = invoices[&#39;Meal Price&#39;].fillna(invoices[&#39;Meal Price&#39;].median()) ## convert the column into integer** invoices[&#39;Meal Price&#39;].astype(int) . # pandas convert to datetime to_datetime print(pd.to_datetime(&#39;2019-8-1&#39;)) print(pd.to_datetime(&#39;2019/8/1&#39;)) print(pd.to_datetime(&#39;8/1/2019&#39;)) print(pd.to_datetime(&#39;Aug, 1 2019&#39;)) print(pd.to_datetime(&#39;Aug - 1 2019&#39;)) print(pd.to_datetime(&#39;August - 1 2019&#39;)) print(pd.to_datetime(&#39;2019, August - 1&#39;)) print(pd.to_datetime(&#39;20190108&#39;)) . source: https://towardsdatascience.com/learn-advanced-features-for-pythons-main-data-analysis-library-in-20-minutes-d0eedd90d086 . Method chaining . https://towardsdatascience.com/the-unreasonable-effectiveness-of-method-chaining-in-pandas-15c2109e3c69 R to python: gist . # method chaining def csnap(df, fn=lambda x: x.shape, msg=None): &quot;&quot;&quot; Custom Help function to print things in method chaining. Returns back the df to further use in chaining. &quot;&quot;&quot; if msg: print(msg) display(fn(df)) return df ( wine.pipe(csnap) .rename(columns={&quot;color_intensity&quot;: &quot;ci&quot;}) .assign(color_filter=lambda x: np.where((x.hue &gt; 1) &amp; (x.ci &gt; 7), 1, 0)) .pipe(csnap) .query(&quot;alcohol &gt; 14&quot;) .pipe(csnap, lambda df: df.head(), msg=&quot;After&quot;) .sort_values(&quot;alcohol&quot;, ascending=False) .reset_index(drop=True) .loc[:, [&quot;alcohol&quot;, &quot;ci&quot;, &quot;hue&quot;]] .pipe(csnap, lambda x: x.sample(5)) ) . good explanation: https://tomaugspurger.github.io/method-chaining.html . assign (0.16.0): For adding new columns to a DataFrame in a chain (inspired by dplyr’s mutate) | pipe (0.16.2): For including user-defined methods in method chains. | rename (0.18.0): For altering axis names (in additional to changing the actual labels as before). | Window methods (0.18): Took the top-level pd.rolling_* and pd.expanding_* functions and made them NDFrame methods with a groupby-like API. | Resample (0.18.0) Added a new groupby-like API | .where/mask/Indexers accept Callables (0.18.1): In the next release you’ll be able to pass a callable to the indexing methods, to be evaluated within the DataFrame’s context (like .query, but with code instead of strings). | . https://www.quora.com/I-love-the-flexibility-of-pandas-dataframes-but-I-feel-like-they-can-make-code-harder-to-read-and-maintain-What-are-some-pandas-best-practices-that-address-this-issue . Tidyverse vs pandas: link . (top) . Binning . # binning pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3, labels=[&quot;bad&quot;, &quot;medium&quot;, &quot;good&quot;]) [bad, good, medium, medium, good, bad] Categories (3, object): [bad &lt; medium &lt; good] # binning into custom intervals bins = [0, 1, 5, 10, 25, 50, 100] labels = [1,2,3,4,5,6] df[&#39;binned&#39;] = pd.cut(df[&#39;percentage&#39;], bins=bins, labels=labels) print (df) percentage binned 0 46.50 5 1 44.20 5 2 100.00 6 3 42.12 5 . source . Clipping (top) . # removing outlier df.clip(lower=pd.Series({&#39;A&#39;: 2.5, &#39;B&#39;: 4.5}), axis=1) Outlier removal python q = df[&quot;col&quot;].quantile(0.99) df[df[&quot;col&quot;] &lt; q] #or df = pd.DataFrame(np.random.randn(100, 3)) from scipy import stats df[(np.abs(stats.zscore(df)) &lt; 3).all(axis=1)] . source . df[&#39;Date of Publication&#39;] = pd.to_numeric(extr) # np.where df[&#39;Place of Publication&#39;] = np.where(london, &#39;London&#39;, np.where(oxford, &#39;Oxford&#39;, pub.str.replace(&#39;-&#39;, &#39; &#39;))) # 1929 1839, 38-54 # 2836 [1897?] regex = r&#39;^( d{4})&#39; extr = df[&#39;Date of Publication&#39;].str.extract(r&#39;^( d{4})&#39;, expand=False) # columns to ditionary master_dict = dict(df.drop_duplicates(subset=&quot;term&quot;)[[&quot;term&quot;,&quot;uid&quot;]].values.tolist()) . pivoting table https://stackoverflow.com/questions/47152691/how-to-pivot-a-dataframe . replace with map . d = {&#39;apple&#39;: 1, &#39;peach&#39;: 6, &#39;watermelon&#39;: 4, &#39;grapes&#39;: 5, &#39;orange&#39;: 2,&#39;banana&#39;: 3} df[&quot;fruit_tag&quot;] = df[&quot;fruit_tag&quot;].map(d) . regex matching groups https://stackoverflow.com/questions/2554185/match-groups-in-python . import re mult = re.compile(&#39;(two|2) (?P&lt;race&gt;[a-z]+) (?P&lt;gender&gt;(?:fe)?male)s&#39;) s = &#39;two hispanic males, 2 hispanic females&#39; mult.sub(r&#39; g&lt;race&gt; g&lt;gender&gt;, g&lt;race&gt; g&lt;gender&gt;&#39;, s) # &#39;hispanic male, hispanic male, hispanic female, hispanic female&#39; . source . (top) . test if type is string is equal . isinstance(s, str) . apply function to column . df[&#39;a&#39;] = df[&#39;a&#39;].apply(lambda x: x + 1) . exploding a column . df = pd.DataFrame([{&#39;var1&#39;: &#39;a,b,c&#39;, &#39;var2&#39;: 1}, {&#39;var1&#39;: &#39;d,e,f&#39;, &#39;var2&#39;: 2}]) df.assign(var1=df.var1.str.split(&#39;,&#39;)).explode(&#39;var1&#39;) . . (top) . Performance . Reshaping dataframe . The similarity between melt and stack: blog post . . sorting dataframe . df = pd.read_csv(&quot;data/347136217_T_ONTIME.csv&quot;) delays = df[&#39;DEP_DELAY&#39;] # Select the 5 largest delays delays.nlargest(5).sort_values() . %timeit delays.sort_values().tail(5) 31 ms ± 1.05 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) %timeit delays.nlargest(5).sort_values() 7.87 ms ± 113 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) . check memory usage: . c = s.astype(&#39;category&#39;) print(&#39;{:0.2f} KB&#39;.format(c.memory_usage(index=False) / 1000)) . (top) . Concat vs. append . source . fast append via list of dictionaries: . rows_list = [] for row in input_rows: dict1 = {} # get input row in dictionary format # key = col_name dict1.update(blah..) rows_list.append(dict1) df = pd.DataFrame(rows_list) . source: link . alternatives . #Append def f1(): result = df for i in range(9): result = result.append(df) return result # Concat def f2(): result = [] for i in range(10): result.append(df) return pd.concat(result) In [101]: %timeit f1() 1 loops, best of 3: 1.66 s per loop In [102]: %timeit f2() 1 loops, best of 3: 220 ms per loop . timings = (pd.DataFrame({&quot;Append&quot;: t_append, &quot;Concat&quot;: t_concat}) .stack() .reset_index() .rename(columns={0: &#39;Time (s)&#39;, &#39;level_1&#39;: &#39;Method&#39;})) timings.head() . (top) . Dataframe: iterate rows . Useful links . how-to-iterate-over-rows-in-a-dataframe-in-pandas: link | how-to-make-your-pandas-loop-71-803-times-faster: link | example of bringing down runtime: iterrows, iloc, get_value, apply: link | complex example using haversine_looping: link, jupyter notebook | different-ways-to-iterate-over-rows-in-a-pandas-dataframe-performance-comparison: link | pandas performance tweaks: cython, using numba | . . Vectorization | Cython routines | List Comprehensions (vanilla for loop) | DataFrame.apply(): i) Reductions that can be performed in cython, ii) Iteration in python space | DataFrame.itertuples() and iteritems() | DataFrame.iterrows() | (top) . Profiling book chapter from jakevdp: link . %timeit sum(range(100)) # single line %timeit np.arange(4)[pd.Series([1, 2, 3])] %timeit np.arange(4)[pd.Series([1, 2, 3]).values] 111 µs ± 2.25 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each) 61.1 µs ± 2.7 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each) %%timeit # full cell total = 0 for i in range(1000): for j in range(1000): total += i * (-1) ** j # profiling def sum_of_lists(N): total = 0 for i in range(5): L = [j ^ (j &gt;&gt; i) for j in range(N)] total += sum(L) return total %prun sum_of_lists(1000000) %load_ext line_profiler %lprun -f sum_of_lists sum_of_lists(5000) # memory usage %load_ext memory_profiler %memit sum_of_lists(1000000) . performance plots (notebook link): . import perfplot import pandas as pd import numpy as np perfplot.show( setup=lambda n: pd.DataFrame(np.random.choice(1000, (n, 2)), columns=[&#39;A&#39;,&#39;B&#39;]), kernels=[ lambda df: df[df.A != df.B], lambda df: df.query(&#39;A != B&#39;), lambda df: df[[x != y for x, y in zip(df.A, df.B)]] ], labels=[&#39;vectorized !=&#39;, &#39;query (numexpr)&#39;, &#39;list comp&#39;], n_range=[2**k for k in range(0, 15)], xlabel=&#39;N&#39; ) . (top) . list comprehension . # iterating over one column - `f` is some function that processes your data result = [f(x) for x in df[&#39;col&#39;]] # iterating over two columns, use `zip` result = [f(x, y) for x, y in zip(df[&#39;col1&#39;], df[&#39;col2&#39;])] # iterating over multiple columns result = [f(row[0], ..., row[n]) for row in df[[&#39;col1&#39;, ...,&#39;coln&#39;]].values] . Further tipps . Do numerical calculations with NumPy functions. They are two orders of magnitude faster than Python’s built-in tools. | Of Python’s built-in tools, list comprehension is faster than map() , which is significantly faster than for. | For deeply recursive algorithms, loops are more efficient than recursive function calls. | You cannot replace recursive loops with map(), list comprehension, or a NumPy function. | “Dumb” code (broken down into elementary operations) is the slowest. Use built-in functions and tools. | . source: example code, link . (top) . Parallel data structures . https://towardsdatascience.com/make-your-own-super-pandas-using-multiproc-1c04f41944a1 | https://github.com/modin-project/modin | https://github.com/jmcarpenter2/swifter | datatable blog post | vaex: talk, blog,github | . https://learning.oreilly.com/library/view/python-high-performance/9781787282896/ . get all combinations from two columns . tuples = [tuple(x) for x in dm_bmindex_df_without_index_df[[&#39;trial&#39;, &#39;biomarker_name&#39;]].values] . (top) . Jupyter notebooks . jupyter notebook best practices (link): structure your notebook, automate jupyter execution: link . Extensions: general, | snippets extension: link | convert notebook. post-save hook: gist | jupyter theme github link: jt -t grade3 -fs 95 -altp -tfs 11 -nfs 115 -cellw 88% -T . From jupyter notebooks to standalone apps (Voila): github, blog (example github PR) . | . jupyter lab: Shortcut to run single command: stackoverflow . Notebooks in production . directory structure, layout, workflow: blog post also: cookiecutter . workflow . . (top) . Directory structure . raw - Contains the unedited csv and Excel files used as the source for analysis. | interim - Used if there is a multi-step manipulation. This is a scratch location and not always needed but helpful to have in place so directories do not get cluttered or as a temp location form troubleshooting issues. | processed - In many cases, I read in multiple files, clean them up and save them to a new location in a binary format. This streamlined format makes it easier to read in larger files later in the processing pipeline. | . (top) . Further link . how netflix runs notebooks: scheduling, integration testing: link . jupyter notebook template . header section . A good name for the notebook (as described above) | A summary header that describes the project | Free form description of the business reason for this notebook. I like to include names, dates and snippets of emails to make sure I remember the context. | A list of people/systems where the data originated. | I include a simple change log. I find it helpful to record when I started and any major changes along the way. I do not update it with every single change but having some date history is very beneficial. | . (top) . Orchestration . https://taskfile.dev/#/ . jupyter code snippets . # jupyter notebook --generate-config jupyter notebook --generate-config # start in screen session screen -d -m -S JUPYTER jupyter notebook --ip 0.0.0.0 --port 8889 --no-browser --NotebookApp.token=&#39;&#39; # install packages in jupyter !pip install package-name # environment variables %%bash which python # reset/set password jupyter notebook password # show all running notebooks jupyter notebook list # (Can be useful to get a hash for a notebook) . append to path . from os.path import dirname sys.path.append(dirname(__file__)) . hide warnings . import warnings warnings.filterwarnings(&#39;ignore&#39;) warnings.filterwarnings(action=&#39;once&#39;) . tqdm (top) . from tqdm import tqdm for i in tqdm(range(10000)): . qqrid (top) . import qqrid qqrid_widget = qqrid.show_grid(df, show_toolbar=True) qqrid_widget . print all numpy . import numpy numpy.set_printoptions(threshold=numpy.nan) . Debugging: . ipdb . # debug ipdb from IPython.core.debugger import set_trace def select_condition(tmp): set_trace() . Pixie debugger . # built-in profiler %prun -l 4 estimate_and_update(100) # line by line profiling pip install line_profiler %load_ext line_profiler %lprun -f sum_of_lists sum_of_lists(5000) # memory usage pip install memory_profiler %load_ext memory_profiler %memit sum_of_lists(1000000) . source: Timing and profiling . add tags to jupyterlab: https://github.com/jupyterlab/jupyterlab/issues/4100 . { &quot;tags&quot;: [ &quot;to_remove&quot; ], &quot;slideshow&quot;: { &quot;slide_type&quot;: &quot;fragment&quot; } } . removing tags: https://groups.google.com/forum/#!topic/jupyter/W2M_nLbboj4 . (top) . Timing and Profiling . https://jakevdp.github.io/PythonDataScienceHandbook/01.07-timing-and-profiling.html . test code . test driven development in jupyter notebook . asserts . def multiplyByTwo(x): return x * 3 assert multiplyByTwo(2) == 4, &quot;2 multiplied by 2 should be equal 4&quot; # test file size assert os.path.getsize(bm_index_master_file) &gt; 150000000, &#39;Output file size should be &gt; 150Mb&#39; # assert is nan assert np.isnan(ret_none), f&quot;Can&#39;t deal with &#39;None values&#39;: {ret_none} == {np.nan}&quot; . Production-ready notebooks: link If tqdm doesnt work: install ipywidgets Hbox full: link . Qgrid . Qgrid readme . qgrid.show_grid(e_tpatt_df, grid_options={&#39;forceFitColumns&#39;: False, &#39;defaultColumnWidth&#39;: 100}) . Debugging conda . # conda show install versions import sys print(sys.path) or import sys, fastai print(sys.modules[&#39;fastai&#39;]) . Running Jupyter . jupyter notebook --browser=false &amp;&gt; /dev/null &amp; --matplotlib inline --port=9777 --browser=false # Check GPU is working GPU working from tensorflow.python.client import device_lib def get_available_devices(): local_device_protos = device_lib.list_local_devices() return [x.name for x in local_device_protos] print(get_available_devices()) . (top) . installing kernels . # conda install kernel source activate &lt;ANACONDA_ENVIRONMENT_NAME&gt; pip install ipykernel python -m ipykernel install --user or source activate myenv python -m ipykernel install --user --name myenv --display-name &quot;Python (myenv)&quot; . source stackoverflow . (top) . ## . unsorted . # use dictionary to count list &gt;&gt;&gt; from collections import Counter &gt;&gt;&gt; Counter([&#39;apple&#39;,&#39;red&#39;,&#39;apple&#39;,&#39;red&#39;,&#39;red&#39;,&#39;pear&#39;]) Counter({&#39;red&#39;: 3, &#39;apple&#39;: 2, &#39;pear&#39;: 1}) . # dictionary keys to list list(dict.keys()) . # dictionary remove nan # if nan in keys clean_dict = filter(lambda k: not isnan(k), my_dict) # if nan in values clean_dict = filter(lambda k: not isnan(my_dict[k]), my_dict) . # list remove nan cleanedList = [x for x in countries if str(x) != &#39;nan&#39;] . # pandas convert all columns to lowercase df.apply(lambda x: x.astype(str).str.lower()) . # pandas set difference tow columns # source: https://stackoverflow.com/questions/18180763/set-difference-for-pandas from pandas import DataFrame df1 = DataFrame({&#39;col1&#39;:[1,2,3], &#39;col2&#39;:[2,3,4]}) df2 = DataFrame({&#39;col1&#39;:[4,2,5], &#39;col2&#39;:[6,3,5]}) print df2[~df2.isin(df1).all(1)] print df2[(df2!=df1)].dropna(how=&#39;all&#39;) print df2[~(df2==df1)].dropna(how=&#39;all&#39;) # union print(&quot;Union :&quot;, A | B) # intersection print(&quot;Intersection :&quot;, A &amp; B) # difference print(&quot;Difference :&quot;, A - B) # symmetric difference print(&quot;Symmetric difference :&quot;, A ^ B) . # pandas value counts to dataframe df = value_counts.rename_axis(&#39;unique_values&#39;).reset_index(name=&#39;counts&#39;) . # python dictionary get first key list(tree_number_dict.keys())[0] . # pandas dataframe get cell value by condition function(df.loc[df[&#39;condition&#39;].isna(),&#39;condition&#39;].values[0],1) . # dataframe drop duplicates keep first df = df.drop_duplicates(cols=&#39;index&#39;,take_last=True)# 2 use new col . (top) .",
            "url": "https://fabsta.github.io/2020/03/20/data_science_snippets.html",
            "relUrl": "/2020/03/20/data_science_snippets.html",
            "date": " • Mar 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "COVID-19 Deaths Per Capita",
            "content": "Deaths Per Million Of Inhabitants . Since reaching at least 1 death per million . Tip: Click (Shift+ for multiple) on countries in the legend to filter the visualization. . Last Available Total Deaths By Country: . date Deaths per Million Log of Deaths per Million . Country . China 2020-03-20 | 2.307882 | 0.363214 | . France 2020-03-20 | 6.693804 | 0.825673 | . Iran 2020-03-20 | 17.655874 | 1.246889 | . Italy 2020-03-20 | 67.924641 | 1.832027 | . South Korea 2020-03-20 | 1.843780 | 0.265709 | . Spain 2020-03-20 | 22.500599 | 1.352194 | . United Kingdom 2020-03-20 | 2.689570 | 0.429683 | . Appendix . . Warning: The following chart, &quot;Cases Per Million of Habitants&quot; is biased depending on how widely a country administers tests. Please read with caution. . Cases Per Million of Habitants . date Cases per Million Log of Cases per Million . Country . Brazil 2020-03-20 | 3.789032 | 0.578528 | . China 2020-03-20 | 57.643841 | 1.760753 | . France 2020-03-20 | 189.300776 | 2.277152 | . Germany 2020-03-20 | 241.712072 | 2.383298 | . Iran 2020-03-20 | 242.032099 | 2.383873 | . Italy 2020-03-20 | 792.134065 | 2.898799 | . Japan 2020-03-20 | 7.553862 | 0.878169 | . Portugal 2020-03-20 | 98.746253 | 1.994521 | . Singapore 2020-03-20 | 67.439220 | 1.828913 | . South Korea 2020-03-20 | 169.706249 | 2.229698 | . Spain 2020-03-20 | 440.304157 | 2.643753 | . United Kingdom 2020-03-20 | 60.651311 | 1.782840 | . United States 2020-03-20 | 58.867136 | 1.769873 | . This analysis was conducted by Joao B. Duarte. Assitance with creating visualizations were provided by Hamel Husain. Relevant sources are listed below: . &quot;2019 Novel Coronavirus COVID-19 (2019-nCoV) Data Repository by Johns Hopkins CSSE&quot; GitHub repository. . | Feenstra, Robert C., Robert Inklaar and Marcel P. Timmer (2015), &quot;The Next Generation of the Penn World Table&quot; American Economic Review, 105(10), 3150-3182 . |",
            "url": "https://fabsta.github.io/covid-compare-permillion/",
            "relUrl": "/covid-compare-permillion/",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": "import autogluon as ag from autogluon import TabularPrediction as task . train_data = task.Dataset(file_path=&#39;https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv&#39;) train_data = train_data.head(500) # subsample 500 data points for faster demo print(train_data.head()) . Loaded data from: https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv | Columns = 15 / 15 | Rows = 39073 -&gt; 39073 . age workclass fnlwgt education education-num marital-status 0 25 Private 178478 Bachelors 13 Never-married 1 23 State-gov 61743 5th-6th 3 Never-married 2 46 Private 376789 HS-grad 9 Never-married 3 55 ? 200235 HS-grad 9 Married-civ-spouse 4 36 Private 224541 7th-8th 4 Married-civ-spouse occupation relationship race sex capital-gain 0 Tech-support Own-child White Female 0 1 Transport-moving Not-in-family White Male 0 2 Other-service Not-in-family White Male 0 3 ? Husband White Male 0 4 Handlers-cleaners Husband White Male 0 capital-loss hours-per-week native-country class 0 0 40 United-States &lt;=50K 1 0 35 United-States &lt;=50K 2 0 15 United-States &lt;=50K 3 0 50 United-States &gt;50K 4 0 40 El-Salvador &lt;=50K . label_column = &#39;class&#39; print(&quot;Summary of class variable: n&quot;, train_data[label_column].describe()) . Summary of class variable: count 500 unique 2 top &lt;=50K freq 394 Name: class, dtype: object . dir = &#39;agModels-predictClass&#39; # specifies folder where to store trained models predictor = task.fit(train_data=train_data, label=label_column, output_directory=dir) . Beginning AutoGluon training ... Preprocessing data ... Here are the first 10 unique label values in your data: [&#39; &lt;=50K&#39; &#39; &gt;50K&#39;] AutoGluon infers your prediction problem is: binary (because only two unique label-values observed) If this is wrong, please specify `problem_type` argument in fit() instead (You may specify problem_type as one of: [&#39;binary&#39;, &#39;multiclass&#39;, &#39;regression&#39;]) Selected class &lt;--&gt; label mapping: class 1 = &gt;50K, class 0 = &lt;=50K Data preprocessing and feature engineering runtime = 0.16s ... AutoGluon will gauge predictive performance using evaluation metric: accuracy To change this, specify the eval_metric argument of fit() /usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/imp.py:342: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working return _load(spec) Fitting model: RandomForestClassifierGini ... 0.69s = Training runtime 0.9 = Validation accuracy score Fitting model: RandomForestClassifierEntr ... 0.7s = Training runtime 0.9 = Validation accuracy score Fitting model: ExtraTreesClassifierGini ... 0.51s = Training runtime 0.87 = Validation accuracy score Fitting model: ExtraTreesClassifierEntr ... 0.48s = Training runtime 0.86 = Validation accuracy score Fitting model: KNeighborsClassifierUnif ... 0.02s = Training runtime 0.8 = Validation accuracy score Fitting model: KNeighborsClassifierDist ... 0.01s = Training runtime 0.77 = Validation accuracy score Fitting model: LightGBMClassifier ... 0.74s = Training runtime 0.88 = Validation accuracy score Fitting model: CatboostClassifier ... 1.11s = Training runtime 0.9 = Validation accuracy score Fitting model: NeuralNetClassifier ... 7.53s = Training runtime 0.87 = Validation accuracy score Fitting model: LightGBMClassifierCustom ... 1.13s = Training runtime 0.89 = Validation accuracy score Fitting model: weighted_ensemble_l1 ... 0.59s = Training runtime 0.9 = Validation accuracy score AutoGluon training complete, total runtime = 16.0s ... . test_data = task.Dataset(file_path=&#39;https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv&#39;) y_test = test_data[label_column] # values to predict test_data_nolab = test_data.drop(labels=[label_column],axis=1) # delete label column to prove we&#39;re not cheating print(test_data_nolab.head()) . Loaded data from: https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv | Columns = 15 / 15 | Rows = 9769 -&gt; 9769 . age workclass fnlwgt education education-num 0 31 Private 169085 11th 7 1 17 Self-emp-not-inc 226203 12th 8 2 47 Private 54260 Assoc-voc 11 3 21 Private 176262 Some-college 10 4 17 Private 241185 12th 8 marital-status occupation relationship race sex 0 Married-civ-spouse Sales Wife White Female 1 Never-married Sales Own-child White Male 2 Married-civ-spouse Exec-managerial Husband White Male 3 Never-married Exec-managerial Own-child White Female 4 Never-married Prof-specialty Own-child White Male capital-gain capital-loss hours-per-week native-country 0 0 0 20 United-States 1 0 0 45 United-States 2 0 1887 60 United-States 3 0 0 30 United-States 4 0 0 20 United-States . predictor = task.load(dir) # unnecessary, just demonstrates how to load previously-trained predictor from file y_pred = predictor.predict(test_data_nolab) print(&quot;Predictions: &quot;, y_pred) perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True) . Evaluation: accuracy on test data: 0.824854 Evaluations on test data: { &#34;accuracy&#34;: 0.8248541304125294, &#34;accuracy_score&#34;: 0.8248541304125294, &#34;balanced_accuracy_score&#34;: 0.7104318244165013, &#34;matthews_corrcoef&#34;: 0.47480025693977573, &#34;f1_score&#34;: 0.8248541304125294 } . Predictions: [&#39; &lt;=50K&#39; &#39; &lt;=50K&#39; &#39; &lt;=50K&#39; ... &#39; &lt;=50K&#39; &#39; &lt;=50K&#39; &#39; &lt;=50K&#39;] . Detailed (per-class) classification report: { &#34; &lt;=50K&#34;: { &#34;precision&#34;: 0.8546712802768166, &#34;recall&#34;: 0.928197557374849, &#34;f1-score&#34;: 0.8899182911921766, &#34;support&#34;: 7451 }, &#34; &gt;50K&#34;: { &#34;precision&#34;: 0.6809779367918902, &#34;recall&#34;: 0.4926660914581536, &#34;f1-score&#34;: 0.5717146433041302, &#34;support&#34;: 2318 }, &#34;accuracy&#34;: 0.8248541304125294, &#34;macro avg&#34;: { &#34;precision&#34;: 0.7678246085343534, &#34;recall&#34;: 0.7104318244165013, &#34;f1-score&#34;: 0.7308164672481534, &#34;support&#34;: 9769 }, &#34;weighted avg&#34;: { &#34;precision&#34;: 0.8134571160636874, &#34;recall&#34;: 0.8248541304125294, &#34;f1-score&#34;: 0.8144145491710391, &#34;support&#34;: 9769 } } .",
            "url": "https://fabsta.github.io/2020/02/25/Untitled.html",
            "relUrl": "/2020/02/25/Untitled.html",
            "date": " • Feb 25, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc: true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://fabsta.github.io/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Footnotes . This is the footnote. &#8617; . |",
            "url": "https://fabsta.github.io/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://fabsta.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}